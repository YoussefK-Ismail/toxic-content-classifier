{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the Scale of Large Language Models: A Deep Dive into Quantization\n",
    "\n",
    "**Author:** Youssef Khaled Ismail  \n",
    "**Course:** Cellula Technologies  \n",
    "**Task:** Task 0 - Research Part\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Large Language Models (LLMs) like **BERT** and **LLaMA** have revolutionized Natural Language Processing. However, their massive size presents significant challenges:\n",
    "\n",
    "### Key Challenges:\n",
    "\n",
    "**Memory Constraints:**\n",
    "- A model like LLaMA-70B requires ~140GB of VRAM just to load in FP16\n",
    "- Most consumer GPUs have 8-24GB VRAM\n",
    "- Makes deployment impossible on standard hardware\n",
    "\n",
    "**Inference Latency:**\n",
    "- Large models require more memory bandwidth\n",
    "- Slows down token generation\n",
    "- Poor user experience in real-time applications\n",
    "\n",
    "**Deployment Costs:**\n",
    "- High-end GPUs (A100/H100) are expensive (\\$10,000-\\$30,000)\n",
    "- Often unavailable due to high demand\n",
    "- Cloud costs can be prohibitive\n",
    "\n",
    "### The Solution: Quantization\n",
    "\n",
    "**Quantization** is a key technique to address these issues by:\n",
    "- Reducing the precision of the model's weights and activations\n",
    "- Effectively shrinking the model size\n",
    "- Maintaining minimal impact on performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Foundation of Quantization\n",
    "\n",
    "Quantization maps high-precision values (typically **FP32** or **FP16**) to lower-precision discrete values (e.g., **INT8**, **INT4**).\n",
    "\n",
    "### 2.1 Linear Quantization\n",
    "\n",
    "The most common form is **linear quantization**, which can be expressed as:\n",
    "\n",
    "$$Q(x) = \\text{round}\\left(\\frac{x}{S} + Z\\right)$$\n",
    "\n",
    "Where:\n",
    "- $x$: The original floating-point value\n",
    "- $S$: The **Scale** factor (a positive floating-point number)\n",
    "- $Z$: The **Zero-point** (an integer ensuring that the floating-point zero maps exactly to a quantized value)\n",
    "\n",
    "### Dequantization Formula:\n",
    "\n",
    "To recover the approximate floating-point value:\n",
    "\n",
    "$$\\hat{x} = S \\cdot (Q(x) - Z)$$\n",
    "\n",
    "### Mathematical Example:\n",
    "\n",
    "Let's say we have:\n",
    "- Original value: $x = 0.75$\n",
    "- Scale: $S = 0.1$\n",
    "- Zero-point: $Z = 0$\n",
    "\n",
    "Then:\n",
    "$$Q(0.75) = \\text{round}\\left(\\frac{0.75}{0.1} + 0\\right) = \\text{round}(7.5) = 8$$\n",
    "\n",
    "Dequantization:\n",
    "$$\\hat{x} = 0.1 \\cdot (8 - 0) = 0.8 \\approx 0.75$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Symmetric vs. Asymmetric Quantization\n",
    "\n",
    "| Feature | Symmetric | Asymmetric |\n",
    "|---------|-----------|------------|\n",
    "| **Zero-point ($Z$)** | Always 0 | Non-zero integer |\n",
    "| **Range** | $[-r, r]$ | $[\\min, \\max]$ |\n",
    "| **Efficiency** | Faster (simpler math) | Better utilization of bit-range |\n",
    "| **Use Case** | Weights (often centered around 0) | Activations (arbitrary range) |\n",
    "| **Formula** | $Q(x) = \\text{round}(x/S)$ | $Q(x) = \\text{round}(x/S + Z)$ |\n",
    "\n",
    "### Visual Comparison:\n",
    "\n",
    "**Symmetric Quantization:**\n",
    "```\n",
    "FP32: [-1.0, -0.5, 0.0, 0.5, 1.0]\n",
    "         â†“     â†“    â†“    â†“    â†“\n",
    "INT8:  [-128, -64,  0,  64, 127]\n",
    "```\n",
    "\n",
    "**Asymmetric Quantization:**\n",
    "```\n",
    "FP32: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "        â†“     â†“    â†“     â†“    â†“\n",
    "INT8:  [0,   64,  128,  192, 255]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Quantization Techniques for LLMs\n",
    "\n",
    "Standard INT8 quantization often causes significant accuracy drops in LLMs due to **\"outlier features.\"** Modern techniques address this:\n",
    "\n",
    "### 3.1 GPTQ (Post-Training Quantization)\n",
    "\n",
    "**Key Innovation:**\n",
    "- Uses second-order information (Hessian matrix) to quantize weights layer-by-layer\n",
    "- Minimizes the reconstruction error\n",
    "- Optimal Trade-off Quantization (OPT)\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "Minimize the reconstruction error:\n",
    "\n",
    "$$\\min_{\\mathbf{W}_q} \\|\\mathbf{W} \\mathbf{X} - \\mathbf{W}_q \\mathbf{X}\\|_2^2$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W}$: Original weights\n",
    "- $\\mathbf{W}_q$: Quantized weights\n",
    "- $\\mathbf{X}$: Calibration data\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… High accuracy (1-2% loss even at 4-bit)\n",
    "- âœ… Works with pre-trained models (no retraining)\n",
    "- âœ… Fast quantization process\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 AWQ (Activation-aware Weight Quantization)\n",
    "\n",
    "**Key Innovation:**\n",
    "- Protects important weights by observing activation magnitudes\n",
    "- Scales weights instead of just rounding them\n",
    "- Identifies \"salient\" weights that are critical for model performance\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Compute activation statistics: $s_i = \\max(|\\mathbf{X}_i|)$\n",
    "2. Scale weights: $\\mathbf{W}'_i = s_i \\cdot \\mathbf{W}_i$\n",
    "3. Quantize scaled weights\n",
    "4. Rescale during inference\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Better preserves model accuracy\n",
    "- âœ… Especially effective for 3-bit and 4-bit quantization\n",
    "- âœ… Minimal overhead\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 bitsandbytes (NF4)\n",
    "\n",
    "**Key Innovation:**\n",
    "- Introduced with QLoRA\n",
    "- Uses a **NormalFloat 4-bit** data type\n",
    "- Optimal for normally distributed weights\n",
    "\n",
    "**NF4 Data Type:**\n",
    "\n",
    "Instead of uniform quantization bins, NF4 uses bins optimized for a normal distribution:\n",
    "\n",
    "```python\n",
    "NF4_BINS = [-1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848,\n",
    "            -0.0911, 0.0, 0.0911, 0.1848, 0.2844, 0.3949,\n",
    "            0.5251, 0.6962, 1.0]\n",
    "```\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "For weights following $\\mathcal{N}(0, \\sigma^2)$:\n",
    "\n",
    "$$\\text{NF4}(w) = \\arg\\min_{q \\in \\text{NF4\\_BINS}} |w - q|$$\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Minimal accuracy loss (<1%)\n",
    "- âœ… Enables fine-tuning with QLoRA\n",
    "- âœ… 4x memory reduction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Coding Examples\n",
    "\n",
    "Let's implement quantization techniques with practical code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate bitsandbytes torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Manual Quantization Implementation\n",
    "\n",
    "Let's implement quantization from scratch to understand the fundamentals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_asymmetric(x, bits=8):\n",
    "    \"\"\"\n",
    "    Asymmetric quantization\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (numpy array)\n",
    "        bits: Number of bits for quantization\n",
    "    \n",
    "    Returns:\n",
    "        Dequantized values, scale, zero_point\n",
    "    \"\"\"\n",
    "    # Define quantization range\n",
    "    qmin = 0\n",
    "    qmax = 2**bits - 1\n",
    "    \n",
    "    # Calculate scale and zero-point\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = qmin - np.round(x_min / scale)\n",
    "    \n",
    "    # Quantize\n",
    "    q_x = np.round(x / scale + zero_point)\n",
    "    q_x = np.clip(q_x, qmin, qmax).astype(np.int32)\n",
    "    \n",
    "    # Dequantize\n",
    "    dq_x = scale * (q_x - zero_point)\n",
    "    \n",
    "    return dq_x, q_x, scale, zero_point\n",
    "\n",
    "\n",
    "def quantize_symmetric(x, bits=8):\n",
    "    \"\"\"\n",
    "    Symmetric quantization (zero-point = 0)\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (numpy array)\n",
    "        bits: Number of bits for quantization\n",
    "    \n",
    "    Returns:\n",
    "        Dequantized values, scale\n",
    "    \"\"\"\n",
    "    # Define quantization range\n",
    "    qmax = 2**(bits - 1) - 1\n",
    "    qmin = -qmax\n",
    "    \n",
    "    # Calculate scale\n",
    "    abs_max = np.abs(x).max()\n",
    "    scale = abs_max / qmax\n",
    "    \n",
    "    # Quantize\n",
    "    q_x = np.round(x / scale)\n",
    "    q_x = np.clip(q_x, qmin, qmax).astype(np.int32)\n",
    "    \n",
    "    # Dequantize\n",
    "    dq_x = scale * q_x\n",
    "    \n",
    "    return dq_x, q_x, scale\n",
    "\n",
    "\n",
    "# Test the quantization functions\n",
    "np.random.seed(42)\n",
    "test_data = np.random.randn(1000) * 0.5  # Simulated weights\n",
    "\n",
    "# 8-bit quantization\n",
    "dq_8bit, q_8bit, scale_8, zp_8 = quantize_asymmetric(test_data, bits=8)\n",
    "error_8bit = np.mean(np.abs(test_data - dq_8bit))\n",
    "\n",
    "# 4-bit quantization\n",
    "dq_4bit, q_4bit, scale_4, zp_4 = quantize_asymmetric(test_data, bits=4)\n",
    "error_4bit = np.mean(np.abs(test_data - dq_4bit))\n",
    "\n",
    "print(f\"8-bit Quantization Error (MAE): {error_8bit:.6f}\")\n",
    "print(f\"4-bit Quantization Error (MAE): {error_4bit:.6f}\")\n",
    "print(f\"\\nCompression ratio (8-bit): {32/8}x\")\n",
    "print(f\"Compression ratio (4-bit): {32/4}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 4-bit Quantization with bitsandbytes\n",
    "\n",
    "Now let's use the professional `bitsandbytes` library for NF4 quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_id = \"facebook/opt-125m\"  # Small model for demonstration\n",
    "\n",
    "# Configure 4-bit quantization with NF4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Use NormalFloat4 data type\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Compute in FP16 for speed\n",
    "    bnb_4bit_use_double_quant=True,       # Double quantization (quantize the quantization constants)\n",
    ")\n",
    "\n",
    "print(\"Loading model with 4-bit NF4 quantization...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "# Load quantized model\n",
    "model_quantized = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically distribute layers\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"\\nModel memory footprint: {model_quantized.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Testing the Quantized Model\n",
    "\n",
    "Let's test if the quantized model still works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "prompt = \"Quantization is a technique that\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_quantized.device)\n",
    "\n",
    "# Generate\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nGenerating...\\n\")\n",
    "\n",
    "outputs = model_quantized.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated: {generated_text}\")\n",
    "print(\"\\nâœ… Quantized model works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Impact of Quantization\n",
    "\n",
    "Let's visualize how quantization affects weight distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample weights (simulating a neural network layer)\n",
    "np.random.seed(42)\n",
    "original_weights = np.random.randn(10000) * 0.15  # Mean=0, Std=0.15\n",
    "\n",
    "# Quantize to 4-bit\n",
    "quantized_4bit, _, _, _ = quantize_asymmetric(original_weights, bits=4)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original weights\n",
    "axes[0].hist(original_weights, bins=50, color='#4285F4', alpha=0.8, edgecolor='black')\n",
    "axes[0].set_title('Original Weights (FP32)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Weight Value', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[0].legend()\n",
    "\n",
    "# Quantized weights\n",
    "axes[1].hist(quantized_4bit, bins=50, color='#EA4335', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_title('Quantized Weights (4-bit)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Weight Value', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('quantization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Visualization saved as 'quantization_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Quantization Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors for different bit widths\n",
    "bit_widths = [2, 3, 4, 6, 8]\n",
    "errors = []\n",
    "\n",
    "for bits in bit_widths:\n",
    "    dq, _, _, _ = quantize_asymmetric(original_weights, bits=bits)\n",
    "    error = np.mean(np.abs(original_weights - dq))\n",
    "    errors.append(error)\n",
    "    print(f\"{bits}-bit: MAE = {error:.6f}\")\n",
    "\n",
    "# Plot error vs bit width\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(bit_widths, errors, marker='o', linewidth=2, markersize=10, color='#0F9D58')\n",
    "plt.xlabel('Bit Width', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "plt.title('Quantization Error vs Bit Width', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(bit_widths)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_vs_bitwidth.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Error analysis saved as 'error_vs_bitwidth.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison Table\n",
    "\n",
    "Let's compare different quantization approaches for LLaMA-7B model:\n",
    "\n",
    "| Model Size | Precision | Memory (Approx) | Accuracy Loss | Inference Speed |\n",
    "|------------|-----------|-----------------|---------------|------------------|\n",
    "| LLaMA-7B | FP16 | 14 GB | 0% (Baseline) | 1.0x |\n",
    "| LLaMA-7B | INT8 | 7 GB | < 1% | 1.5-2x |\n",
    "| LLaMA-7B | 4-bit (GPTQ) | 3.5 GB | ~1-2% | 2-3x |\n",
    "| LLaMA-7B | 4-bit (NF4) | 3.5 GB | ~1% | 2-3x |\n",
    "| LLaMA-7B | 3-bit (AWQ) | 2.6 GB | ~2-3% | 3-4x |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Memory Reduction:**\n",
    "   - FP16 â†’ INT8: 2x reduction\n",
    "   - FP16 â†’ 4-bit: 4x reduction\n",
    "   - FP16 â†’ 3-bit: 5.3x reduction\n",
    "\n",
    "2. **Accuracy vs Compression:**\n",
    "   - 8-bit: Minimal accuracy loss (<1%)\n",
    "   - 4-bit: Acceptable loss (1-2%)\n",
    "   - 3-bit: Noticeable but usable (2-3%)\n",
    "\n",
    "3. **Speed Improvements:**\n",
    "   - Lower precision = faster inference\n",
    "   - Less memory bandwidth required\n",
    "   - Better cache utilization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Memory Calculation\n",
    "\n",
    "Let's calculate the actual memory requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_memory(num_parameters, precision_bits):\n",
    "    \"\"\"\n",
    "    Calculate model memory in GB\n",
    "    \n",
    "    Args:\n",
    "        num_parameters: Number of model parameters (in billions)\n",
    "        precision_bits: Bit precision (32, 16, 8, 4, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Memory in GB\n",
    "    \"\"\"\n",
    "    bytes_per_param = precision_bits / 8\n",
    "    total_bytes = num_parameters * 1e9 * bytes_per_param\n",
    "    total_gb = total_bytes / 1e9\n",
    "    return total_gb\n",
    "\n",
    "# Example: LLaMA models\n",
    "models = {\n",
    "    \"LLaMA-7B\": 7,\n",
    "    \"LLaMA-13B\": 13,\n",
    "    \"LLaMA-70B\": 70,\n",
    "}\n",
    "\n",
    "precisions = {\n",
    "    \"FP32\": 32,\n",
    "    \"FP16\": 16,\n",
    "    \"INT8\": 8,\n",
    "    \"4-bit\": 4,\n",
    "}\n",
    "\n",
    "print(\"Memory Requirements (GB):\\n\")\n",
    "print(f\"{'Model':<15} {'FP32':<10} {'FP16':<10} {'INT8':<10} {'4-bit':<10}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, num_params in models.items():\n",
    "    row = f\"{model_name:<15}\"\n",
    "    for prec_name, prec_bits in precisions.items():\n",
    "        memory_gb = calculate_model_memory(num_params, prec_bits)\n",
    "        row += f\"{memory_gb:<10.1f}\"\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: Actual memory usage may be higher due to:\")\n",
    "print(\"   - Activation memory\")\n",
    "print(\"   - Optimizer states (during training)\")\n",
    "print(\"   - KV cache (during inference)\")\n",
    "print(\"   - Framework overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### Summary of Key Points:\n",
    "\n",
    "1. **Quantization is Essential:**\n",
    "   - Enables deployment of large models on consumer hardware\n",
    "   - Reduces memory requirements by 2-8x\n",
    "   - Speeds up inference\n",
    "\n",
    "2. **Modern Techniques are Effective:**\n",
    "   - GPTQ: Optimal for post-training quantization\n",
    "   - AWQ: Best for activation-aware quantization\n",
    "   - NF4: Enables efficient fine-tuning with QLoRA\n",
    "\n",
    "3. **Trade-offs Exist:**\n",
    "   - Lower precision = smaller models but potential accuracy loss\n",
    "   - 4-bit quantization offers the best balance\n",
    "   - 8-bit is nearly lossless\n",
    "\n",
    "4. **Practical Applications:**\n",
    "   - Run LLaMA-7B on consumer GPUs (RTX 3090/4090)\n",
    "   - Deploy models in production with lower costs\n",
    "   - Fine-tune large models with QLoRA\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "- **Mixed Precision:** Combine different precisions in one model\n",
    "- **2-bit and 1-bit:** Extreme quantization research\n",
    "- **Hardware Support:** Specialized chips for quantized models\n",
    "- **Quantization-Aware Training:** Train models with quantization in mind\n",
    "\n",
    "---\n",
    "\n",
    "## References:\n",
    "\n",
    "1. Frantar, E., et al. (2023). \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\"\n",
    "2. Lin, J., et al. (2023). \"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\"\n",
    "3. Dettmers, T., et al. (2023). \"QLoRA: Efficient Finetuning of Quantized LLMs\"\n",
    "4. Hugging Face Transformers Documentation\n",
    "5. bitsandbytes Library Documentation\n",
    "\n",
    "---\n",
    "\n",
    "**End of Research Report**\n",
    "\n",
    "**Author:** Youssef Khaled Ismail  \n",
    "**Course:** Cellula Technologies  \n",
    "**Date:** February 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
